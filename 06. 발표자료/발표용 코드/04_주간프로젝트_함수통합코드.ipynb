{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"ZRspdOcdvgIE"},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","import matplotlib.pyplot as plt"]},{"cell_type":"markdown","metadata":{"id":"tmOP_ahdvgIH"},"source":["# 데이터 수집 단계 함수\n","- [수정본]구석구석_크롤링_코드.ipynb\n","- 네이버_리뷰_크롤링_최종_배포용.ipynb"]},{"cell_type":"markdown","metadata":{"id":"qIKudzYGvgII"},"source":["[수정본]구석구석_크롤링_코드.ipynb"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7p3YCLGKvgIJ"},"outputs":[],"source":["def button_click():\n","    # \"인기순\" 버튼 클릭\n","    popular_button = WebDriverWait(driver, 20).until(EC.element_to_be_clickable((By.XPATH, '//button[contains(text(), \"인기순\")]')))\n","    driver.execute_script(\"arguments[0].click();\", popular_button)\n","\n","    # 서울 버튼 클릭\n","    seoul_button = WebDriverWait(driver, 20).until(EC.element_to_be_clickable((By.XPATH, '//*[@id=\"1\"]/button/span')))\n","    driver.execute_script(\"arguments[0].click();\", seoul_button)\n","\n","    # 서울 버튼 클릭 후, 해당 지역의 데이터가 로드될 때까지 대기\n","    wait = WebDriverWait(driver, 30)\n","    wait.until(EC.presence_of_element_located((By.XPATH, '//*[@id=\"contents\"]/div[2]/div[1]/ul/li[1]/div[2]/div/a')))\n","\n","\n","# 각 페이지 데이터 추출 함수\n","# 이동한 페이지의 데이터를 추출하기 위해, 이동한 페이지의 html 소스를 가져와 파싱\n","def extract_data(soup, attraction_list, location_list, tag_list):\n","    for i in range(10):\n","        attraction = soup.select_one(f'#contents > div.wrap_contView.clfix > div.box_leftType1 > ul > li:nth-child({i+1}) > div.area_txt.catchphrase > div > a')\n","        location = soup.select_one(f'#contents > div.wrap_contView.clfix > div.box_leftType1 > ul > li:nth-child({i+1}) > div.area_txt.catchphrase > p:nth-child(2)')\n","        tag = soup.select_one(f'#contents > div.wrap_contView.clfix > div.box_leftType1 > ul > li:nth-child({i+1}) > div.area_txt.catchphrase > p.tag')\n","\n","        # 명소, 위치, 해쉬 태그 3개를 다 가진 것만 추출\n","        if attraction and location and tag:\n","            attraction_list.append(attraction.text)\n","            location_list.append(location.text)\n","            tag_list.append(tag.text)\n","\n","# 각 페이지 스크롤 내리는 함수\n","def slow_scroll_by(offset, duration):\n","    steps = 30  # 스크롤을 나누는 단계 수\n","    step_duration = duration / steps  # 각 단계별 대기 시간\n","    step_size = offset / steps  # 각 단계별 스크롤 크기\n","    for _ in range(steps):\n","        driver.execute_script(f\"window.scrollBy(0, {step_size})\") # steps 동안 step_size 만큼 반복해서 스크롤\n","        time.sleep(step_duration) # 1회 스크롤 하고 step_duration 만큼 대기\n","\n","# 다음 페이지로 넘어가는 함수\n","def next_page(page_number):\n","    # 페이지 바에서 모든 페이지 링크 요소를 찾음\n","    page_bar = driver.find_elements(By.CSS_SELECTOR, '.page_box a')\n","\n","    # 찾은 페이지 링크 요소들 중에서 지정된 페이지 번호(page_number)와 일치하는 버튼을 클릭\n","    for button in page_bar:\n","        if button.get_attribute(\"id\") == str(page_number):\n","            # element.click() 기본 클릭 메소드\n","            # 동적 콘텐츠로 로드하는 경우, 기본 클릭 메소드가 작동되지 않음\n","            # 자바스크립트를 사용하여 클릭 이벤트 실행\n","            driver.execute_script(\"arguments[0].click();\", button)\n","\n","            break\n","    # 페이지가 로드되기 전에 대기\n","    wait = WebDriverWait(driver, 20)\n","    # 페이지의 첫 번째 링크 요소가 새로운 페이지 로딩으로 인해 사라질 때까지 대기\n","    wait.until(EC.staleness_of(page_bar[0]))\n","    # 새 페이지가 로드된 후, 특정 요소가 존재하는지 확인하여 페이지 로딩 완료를 확인\n","    wait.until(EC.presence_of_element_located((By.XPATH, '//*[@id=\"contents\"]/div[2]/div[1]/ul/li[1]/div[2]/div/a')))\n","    # 페이지 로딩 후 잠시 대기\n","    time.sleep(1.5)\n","\n","# 한 면에서 넘길 수 있는 페이지 수는 5개\n","# 그 이후에는 다음 버튼을 클릭해야함.\n","# 다음 버튼 클릭 함수\n","def click_next_button():\n","    # CSS 셀렉터.btn_next.ico를 사용하여 다음 버튼을 찾음\n","    # CSS 셀렉터는 <a> 태그의 class 속성이 btn_next와 ico를 가진 요소를 찾음\n","    next_button = driver.find_element(By.CSS_SELECTOR, '.btn_next.ico')\n","    # 자바스크립트를 사용하여 클릭 이벤트 실행\n","    driver.execute_script(\"arguments[0].click();\", next_button)\n","    # 페이지 로드 대기\n","    wait = WebDriverWait(driver, 20)\n","    wait.until(EC.staleness_of(next_button))\n","    # 새로운 페이지 로드 대기:\n","    wait.until(EC.presence_of_element_located((By.XPATH, '//*[@id=\"contents\"]/div[2]/div[1]/ul/li[1]/div[2]/div/a')))\n","    time.sleep(1.5)"]},{"cell_type":"markdown","metadata":{"id":"ZAh5fEzIvgIK"},"source":["네이버_리뷰_크롤링_최종_배포용.ipynb"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZpBr1ZJAvgIK"},"outputs":[],"source":["def extract_data():\n","    try:\n","        tourist_spot = spot\n","\n","        # 사용 가능한 '이런 점이 좋았어요' 요소 개수 파악\n","        # MHaAm : '이런 점이 좋았어요' 요소 CLASS NAME\n","        # 따라서 MHaAm 로 접근하여 각 요소 리스트에 담고 길이 구하기\n","        good_points_elements = driver.find_elements(By.CLASS_NAME, 'MHaAm')\n","        num_good_points = len(good_points_elements)\n","\n","        # 첫번째 부터 마지막 요소번째 까지 반복\n","        for j in range(0, num_good_points + 1):\n","            try:\n","                # 좋은 점과 선택한 사람 수 추출\n","                good_point = driver.find_elements(By.CLASS_NAME, 't3JSf')[j].text # 이런 점이 좋았어요 - 이런점 추출\n","                selected_people = driver.find_elements(By.CLASS_NAME, 'CUoLy')[j].text # 이런 점이 좋았어요 - 투표 수 추출\n","                spot_attribute = driver.find_element(By.CLASS_NAME, 'lnJFt').text # 관광지 속성(네이버 지도 - 관광지명 옆 작은 카테고리명)\n","\n","                # 추출된 요소들 리스트에 넣기 - 후반부 DataFrame 생성시 사용\n","                good_point_list.append(good_point)\n","                selected_people_list.append(selected_people)\n","                spot_attribute_list.append(spot_attribute)\n","                tourist_spot_list.append(tourist_spot)\n","\n","            except Exception as e:\n","                print(f\"항목 {j}은(는) 존재하지 않습니다. 오류: {e}\")\n","                if 'driver' in locals():\n","                    driver.quit()\n","                return None\n","\n","    except Exception as e:\n","        print(f\"데이터 추출 중 오류 발생: {e}\")"]},{"cell_type":"markdown","source":["## 전처리 & ERD"],"metadata":{"id":"5pFL7rpdvhtN"}},{"cell_type":"code","source":["# DB pivot table 변환 함수\n","\n","def making_pivot(df):\n","    pivot_df = df.pivot_table(\n","        index='Attraction',\n","        columns='Category_Map',\n","        values='Selected_People',\n","        aggfunc='sum',\n","        fill_value=0\n","    )\n","\n","    first_values = df.groupby('Attraction').first()[['Participants', 'Address', 'Tag', 'Longitude', 'Latitude']]\n","    pivot_df = pivot_df.reset_index().merge(first_values, on='Attraction', how='left')\n","\n","    return pivot_df"],"metadata":{"id":"XhjdIDxqvlG0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 실루엣 점수 구하는 함수\n","\n","def silhouetteviz(n_cluster, x_features):\n","    kmeans = KMeans(n_clusters=n_cluster, random_state=42)\n","    y_labels = kmeans.fit_predict(x_features)\n","    silhouette_values = silhouette_samples(x_features, y_labels, metric='euclidean')\n","\n","    # 초기화\n","    y_ax_lower, y_ax_upper = 0, 0\n","    y_ticks=[]\n","\n","    for c in range(n_cluster):\n","        c_silhouette = silhouette_values[y_labels == c]\n","        c_silhouette.sort()\n","        y_ax_upper += len(c_silhouette)\n","        color = cm.jet(float(c)/n_cluster)\n","        plt.barh(range(y_ax_lower, y_ax_upper), c_silhouette,\n","                 height=1.0, edgecolor='none', color=color)\n","        y_ticks.append((y_ax_lower + y_ax_upper) / 2.)\n","        y_ax_lower += len(c_silhouette)\n","    silhouetter_avg = np.mean(silhouette_values)\n","    # print(silhouetter_avg)\n","\n","    # 그래프 꾸며주는 코드\n","    plt.axvline(silhouetter_avg, color='red', linestyle='--') # 실루엣 평균 점수 선 표시\n","    plt.title('Number of Cluster: '+str(n_cluster)+'\\n'\\\n","              + 'Silhouette_score: ' +str(round(silhouetter_avg, 3)))\n","\n","    plt.yticks(y_ticks, range(n_cluster))\n","    plt.xticks([0,0.2,0.4,0.6,0.8,1.0])\n","    plt.ylabel('Cluster')\n","    plt.xlabel('Silhouette coefficient')\n","    plt.tight_layout()\n","    plt.show()"],"metadata":{"id":"ckqTZNRHvyZV"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 군집 분석 후 시각화해주는 함수\n","\n","def clusterScatter(n_cluster, x_features, att1, att2):\n","    c_colors = []\n","    kmeans = KMeans(n_clusters=n_cluster, random_state=42)\n","    y_labels = kmeans.fit_predict(x_features)\n","\n","    for i in range(n_cluster):\n","        c_color = cm.jet(float(i)/n_cluster)\n","        c_colors.append(c_color)\n","        # 클러스터 색상 설정\n","\n","        # cluster 내 데이터 분포를 표현, centroid 동그라미로 시각화\n","        plt.scatter(x_features[y_labels ==i, att1], x_features[y_labels ==i, att2],\n","                    marker ='o', color=c_color, edgecolor='black' , s=50,\n","                    label='cluster'+str(i)\n","                    )\n","        # 각 클러스터의 중심점(centroid) 별(*)로 표시\n","    for i in range(n_cluster):\n","        plt.scatter(kmeans.cluster_centers_[i,att1],kmeans.cluster_centers_[i,att2],\n","                    marker='*', color=c_colors[i], edgecolor='w', s=100)\n","\n","    plt.legend()\n","    plt.grid()\n","    plt.tight_layout()\n","    plt.show()"],"metadata":{"id":"NW7XSTb8wEld"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 텍스트 전처리 함수\n","\n","def make_newTagCols(df, nlp):\n","    # tag 분리한거 담는 컬럼 만들기\n","    df[['newTag', 'newTag_list', 'tag_word_N']] = None\n","    # 모든 태그 모음집\n","    total = ''\n","    # 모든 태그의 명사 추출 모음집\n","    N_list = []\n","\n","    # '#' 분리해서 컬럼에 담기\n","    for i, tag in enumerate(df.Tag):\n","        tag = tag.split('#')[1:]\n","        df['newTag_list'][i] = tag\n","\n","        words = ''\n","        for t in tag:\n","            words = words + t + ' '\n","\n","        total = total + words + ' '\n","        df['newTag'][i] = words\n","\n","        # 명사 추출해서 또 새로운 컬럼에 담기\n","        newTag_N = list(set(nlp.nouns(words)))\n","        N_list.append(newTag_N)\n","        df['tag_word_N'][i] = newTag_N\n","\n","    # '#'제거 해시태그 한 리스트에 모은 것 : total\n","    # total에서 명사 추출한 결과\n","    total_nouns = nlp.nouns(total)\n","\n","    return df, total, N_list, total_nouns"],"metadata":{"id":"yO0XsCStwMBN"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# '관광' 카테고리 관광지의 속성별 총비율 구하는 함수\n","\n","def cluster_ratio_tour(df, cluster_num):\n","    # 관광지별 속성 합계\n","    df['합계'] = df['만족도'] + df['매력도'] + df['편의']\n","\n","    # 클러스터 그룹화\n","    for i in range(1, cluster_num+1):\n","        cond = df['class'] == i\n","        df_cond = df[cond]\n","\n","        총합계 = df_cond['합계'].sum()\n","        df_cond['합계/총합계'] = df_cond['합계'] / 총합계 * 100\n","\n","        # 관광지별 속성 비율\n","        df_cond['만족도 비율'] = df_cond['만족도'] / df_cond['합계']\n","        df_cond['매력도 비율'] = df_cond['매력도'] / df_cond['합계']\n","        df_cond['편의 비율'] = df_cond['편의'] / df_cond['합계']\n","\n","        # 총 관광지를 고려한 관광지별 속성 비율\n","        df_cond['만족도 총비율'] = df_cond['합계/총합계'] * df_cond['만족도 비율']\n","        df_cond['매력도 총비율'] = df_cond['합계/총합계'] * df_cond['매력도 비율']\n","        df_cond['편의 총비율'] = df_cond['합계/총합계'] * df_cond['편의 비율']\n","\n","        df_cond.to_csv(f'./ratio/df_tour_cond{i}.csv')"],"metadata":{"id":"B0vhCjN13RFh"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 모델링"],"metadata":{"id":"vCxfhr04zFDm"}},{"cell_type":"code","source":["# LDA 응집도 구하고 시각화하는 함수\n","\n","def coherence_scores(corpus, dictionary, texts, min_topics, max_topics):\n","\n","    coherence_scores = []\n","\n","    for num_topics in range(min_topics, max_topics):\n","        model = LdaModel(corpus=corpus, num_topics=num_topics, id2word=dictionary, random_state=42)\n","        coherence = CoherenceModel(model=model, texts=texts, dictionary=dictionary)\n","        coherence_scores.append(coherence.get_coherence())\n","\n","    x = [i for i in range(min_topics, max_topics)]\n","\n","    plt.figure(figsize=(10,6))\n","    plt.plot(x, coherence_score)\n","    # x축 값 : 최소~최대 범위의 수\n","    # y축 값 : 반복문으로 추출한 coherence_score\n","    plt.title('Tour')\n","    plt.xlabel('Number of Topics')\n","    plt.ylabel('Coherence Scores')\n","    plt.show()\n","\n","    return coherence_scores"],"metadata":{"id":"SBvB_TAazF0o"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# topic 매핑 함수\n","\n","def max_count(x, y):\n","    # 최빈 단어 유형 추출 후 새로운 컬럼에 추가\n","    x['Max'] = x.max(axis=1)\n","    x['Max_Column'] = x.idxmax(axis=1)\n","\n","    # temp가 아닌 원래 데이터에 컬럼명 변경하여 값 넣기\n","    y['max'] = x['Max']\n","\n","    # temp가 아닌 원래 데이터에 컬럼명 변경하여 값 넣기\n","    y['topic_type'] = x['Max_Column']"],"metadata":{"id":"hhrvwTyMzk7A"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 웹구현 & 콘텐츠 기반 유사도 관련 함수\n","\n","\n","\n","# 해시태그 맵핑 함수\n","def map_hashtags(hashtags, category):\n","    tour_hashtag_mapping = {\n","        '#분위기': '매력도', '#시설': '편의', '#활동': '매력도',\n","        '#접근성': '편의', '#서비스': '만족도', '#가격': '만족도'\n","    }\n","    food_hashtag_mapping = {\n","        '#음식의 맛과 질': '음식의 속성', '#음식의 다양성': '음식의 속성',\n","        '#시설 및 환경': '시설 및 분위기', '#분위기': '시설 및 분위기',\n","        '#청결도와 위생': '시설 및 분위기', '#접근성': '시설 및 분위기',\n","        '#특별한 목적': '시설 및 분위기', '#가격과 가치': '가격 및 서비스',\n","        '#서비스': '가격 및 서비스'\n","    }\n","\n","    if category == '관광':\n","        hashtag_mapping = tour_hashtag_mapping\n","    else:\n","        hashtag_mapping = food_hashtag_mapping\n","\n","    mapped_hashtags = set(hashtag_mapping.get(hashtag, '')\n","                          for hashtag in hashtags if hashtag in hashtag_mapping)\n","\n","    return list(mapped_hashtags)  # 결과를 다시 리스트로 반환하여 순서가 있는 데이터 구조 제공\n","\n","\n","\n","# 피봇테이블 출력 함수\n","def making_pivot(category, popularity_class, types, hashtags, all_data):\n","    mapped_hashtags = map_hashtags(hashtags, category)\n","\n","    # 데이터 필터링\n","    filtered_data = all_data[(all_data[\"Category\"] == category) &\n","                             (all_data[\"class\"] == popularity_class) &\n","                             (all_data[\"type\"].isin(types)) &\n","                             (all_data[\"Category_Map\"].isin(mapped_hashtags))]\n","\n","    if not filtered_data.empty:\n","        pivot_table = filtered_data.pivot_table(\n","            index='Attraction', columns='Category_Map', values='Selected_People', aggfunc='sum', fill_value=0)\n","\n","    if category == '관광':\n","        pivot_table = pd.merge(pivot_table, tour_ratio)\n","    else:\n","        pivot_table = pd.merge(pivot_table, food_ratio)\n","        return pivot_table.drop_duplicates()\n","\n","\n","\n","# 대분류 피봇 테이블 출력 함수\n","def making_pivot_min(df, category):\n","    if category == '관광':\n","        df = df[df['Category'] == '관광']\n","    elif category == '음식':\n","        df = df[df['Category'] == '음식']\n","\n","    return df.pivot_table(index='Attraction', columns='Category_Map', values='Selected_People', aggfunc='sum', fill_value=0).reset_index()\n","\n","\n","\n","# 유사도 함수\n","def cosine_similarity_func(df, df2, category, types, attraction, top_n=5):\n","\n","    condition = (df['Category'] == category) & (df['type'].isin(types))\n","    df_con = df[condition]\n","    temp = making_pivot_min(df_con, category)\n","    if category == '관광':\n","        feature_data = pd.merge(temp, df2).iloc[:, 7:10].values\n","    else:\n","        feature_data = pd.merge(temp, df2).iloc[:, 10:13].values\n","    cosine_sim = cosine_similarity(feature_data)\n","    index = temp[temp['Attraction'] == attraction].index[0]\n","    cosine_sim[index, index] = -np.inf\n","    most_similar_indices = np.argsort(cosine_sim[index])[-top_n:]\n","    recommendations = temp.iloc[most_similar_indices]['Attraction'].values.tolist(\n","    )\n","    return recommendations\n","\n","\n","\n","\n","# 랜덤 테마여행\n","def random_tour(df, topic):\n","    random.seed(42)\n","    tp1 = df[df['topic_type'] == topic]\n","    tp1_lst = tp1[tp1['max'] >= 0.95]['Attraction'].values\n","    shuffle(tp1_lst)\n","    tp1_lst = tp1_lst[:5].tolist()\n","    return tp1_lst"],"metadata":{"id":"B3VzJj_l3vOa"},"execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"base","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.7"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":0}